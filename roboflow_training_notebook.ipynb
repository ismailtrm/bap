{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Balloon Detection Training with Roboflow Dataset\n",
        "\n",
        "This notebook trains a YOLO model on the Roboflow balloon detection dataset with a 90/5/5 train/test/validation split.\n",
        "\n",
        "## Project Context\n",
        "This is part of an air defense demo system that detects and tracks balloon targets with friend/foe discrimination by color.\n",
        "\n",
        "## Dataset Split Strategy\n",
        "- **Training**: 90%\n",
        "- **Test**: 5% \n",
        "- **Validation**: 5%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install roboflow ultralytics matplotlib seaborn pandas numpy opencv-python pillow\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# YOLO and Roboflow\n",
        "from ultralytics import YOLO\n",
        "from roboflow import Roboflow\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check CUDA availability\n",
        "print(\"üîç System Information:\")\n",
        "print(f\"  ‚Ä¢ PyTorch version: {torch.__version__}\")\n",
        "print(f\"  ‚Ä¢ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  ‚Ä¢ CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"  ‚Ä¢ GPU count: {torch.cuda.device_count()}\")\n",
        "    print(f\"  ‚Ä¢ Current GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  ‚Ä¢ GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"  ‚Ä¢ Using CPU (no CUDA available)\")\n",
        "\n",
        "print(\"\\n‚úÖ All packages imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download Roboflow Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Roboflow and download dataset\n",
        "rf = Roboflow(api_key=\"J82Hqb1HfGHJDIfUt6PJ\")\n",
        "project = rf.workspace(\"hss-1e8zb\").project(\"balloon-detection-hyqqa\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"yolov11\")\n",
        "\n",
        "print(f\"‚úÖ Dataset downloaded to: {dataset.location}\")\n",
        "print(f\"Dataset info: {dataset}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Analyze Original Dataset Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore the downloaded dataset structure\n",
        "dataset_path = Path(dataset.location)\n",
        "print(f\"Dataset location: {dataset_path}\")\n",
        "print(\"\\nDataset structure:\")\n",
        "\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    level = root.replace(str(dataset_path), '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for file in files[:5]:  # Show first 5 files\n",
        "        print(f\"{subindent}{file}\")\n",
        "    if len(files) > 5:\n",
        "        print(f\"{subindent}... and {len(files) - 5} more files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the original data.yaml file\n",
        "data_yaml_path = dataset_path / \"data.yaml\"\n",
        "if data_yaml_path.exists():\n",
        "    with open(data_yaml_path, 'r') as f:\n",
        "        original_config = f.read()\n",
        "    print(\"Original data.yaml:\")\n",
        "    print(original_config)\n",
        "else:\n",
        "    print(\"No data.yaml found in dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Custom 90/5/5 Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create custom split directories\n",
        "custom_dataset_path = Path(\"data/roboflow_custom\")\n",
        "custom_dataset_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Create split directories\n",
        "splits = ['train', 'test', 'val']\n",
        "for split in splits:\n",
        "    (custom_dataset_path / split / 'images').mkdir(parents=True, exist_ok=True)\n",
        "    (custom_dataset_path / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Created custom dataset directory structure\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find all images and labels from the original dataset\n",
        "def find_dataset_files(dataset_path):\n",
        "    \"\"\"Find all image and label files in the dataset\"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    \n",
        "    # Look for images in train/val/test directories\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        split_path = dataset_path / split\n",
        "        if split_path.exists():\n",
        "            img_path = split_path / 'images'\n",
        "            lbl_path = split_path / 'labels'\n",
        "            \n",
        "            if img_path.exists():\n",
        "                for img_file in img_path.glob('*.jpg'):\n",
        "                    images.append(img_file)\n",
        "            if lbl_path.exists():\n",
        "                for lbl_file in lbl_path.glob('*.txt'):\n",
        "                    labels.append(lbl_file)\n",
        "    \n",
        "    return images, labels\n",
        "\n",
        "all_images, all_labels = find_dataset_files(dataset_path)\n",
        "print(f\"Found {len(all_images)} images and {len(all_labels)} labels\")\n",
        "\n",
        "# Create image-label pairs\n",
        "image_label_pairs = []\n",
        "for img_path in all_images:\n",
        "    # Find corresponding label file\n",
        "    label_name = img_path.stem + '.txt'\n",
        "    label_path = img_path.parent.parent / 'labels' / label_name\n",
        "    \n",
        "    if label_path.exists():\n",
        "        image_label_pairs.append((img_path, label_path))\n",
        "    else:\n",
        "        print(f\"Warning: No label found for {img_path.name}\")\n",
        "\n",
        "print(f\"Created {len(image_label_pairs)} image-label pairs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data: 90% train, 5% test, 5% val\n",
        "random.shuffle(image_label_pairs)\n",
        "\n",
        "total_samples = len(image_label_pairs)\n",
        "train_size = int(0.90 * total_samples)\n",
        "test_size = int(0.05 * total_samples)\n",
        "val_size = total_samples - train_size - test_size\n",
        "\n",
        "train_pairs = image_label_pairs[:train_size]\n",
        "test_pairs = image_label_pairs[train_size:train_size + test_size]\n",
        "val_pairs = image_label_pairs[train_size + test_size:]\n",
        "\n",
        "print(f\"Dataset split:\")\n",
        "print(f\"  Training: {len(train_pairs)} samples ({len(train_pairs)/total_samples*100:.1f}%)\")\n",
        "print(f\"  Test: {len(test_pairs)} samples ({len(test_pairs)/total_samples*100:.1f}%)\")\n",
        "print(f\"  Validation: {len(val_pairs)} samples ({len(val_pairs)/total_samples*100:.1f}%)\")\n",
        "print(f\"  Total: {total_samples} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy files to custom split directories\n",
        "def copy_files(pairs, split_name):\n",
        "    \"\"\"Copy image and label files to the specified split directory\"\"\"\n",
        "    split_path = custom_dataset_path / split_name\n",
        "    \n",
        "    for img_path, lbl_path in pairs:\n",
        "        # Copy image\n",
        "        img_dest = split_path / 'images' / img_path.name\n",
        "        shutil.copy2(img_path, img_dest)\n",
        "        \n",
        "        # Copy label\n",
        "        lbl_dest = split_path / 'labels' / lbl_path.name\n",
        "        shutil.copy2(lbl_path, lbl_dest)\n",
        "\n",
        "# Copy files for each split\n",
        "copy_files(train_pairs, 'train')\n",
        "copy_files(test_pairs, 'test')\n",
        "copy_files(val_pairs, 'val')\n",
        "\n",
        "print(\"‚úÖ Files copied to custom split directories\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create Custom data.yaml Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create custom data.yaml for our 90/5/5 split\n",
        "data_yaml_content = f\"\"\"# Custom Roboflow Balloon Detection Dataset - 90/5/5 Split\n",
        "# Generated for air defense demo system\n",
        "\n",
        "# Dataset paths (relative to this file)\n",
        "path: {custom_dataset_path.absolute()}\n",
        "train: train/images\n",
        "val: val/images\n",
        "test: test/images\n",
        "\n",
        "# Classes\n",
        "nc: 1  # number of classes\n",
        "names: ['balloon']  # class names\n",
        "\n",
        "# Dataset info\n",
        "total_samples: {total_samples}\n",
        "train_samples: {len(train_pairs)}\n",
        "val_samples: {len(val_pairs)}\n",
        "test_samples: {len(test_pairs)}\n",
        "split_ratio: \"90/5/5\"\n",
        "\"\"\"\n",
        "\n",
        "# Write the custom data.yaml\n",
        "custom_data_yaml = custom_dataset_path / \"data.yaml\"\n",
        "with open(custom_data_yaml, 'w') as f:\n",
        "    f.write(data_yaml_content)\n",
        "\n",
        "print(\"‚úÖ Created custom data.yaml\")\n",
        "print(\"\\nCustom data.yaml content:\")\n",
        "print(data_yaml_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Dataset Analysis and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze dataset statistics\n",
        "def analyze_dataset(split_pairs, split_name):\n",
        "    \"\"\"Analyze a dataset split\"\"\"\n",
        "    total_objects = 0\n",
        "    image_sizes = []\n",
        "    \n",
        "    for img_path, lbl_path in split_pairs:\n",
        "        # Count objects in label file\n",
        "        with open(lbl_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            total_objects += len([line for line in lines if line.strip()])\n",
        "        \n",
        "        # Get image dimensions\n",
        "        img = Image.open(img_path)\n",
        "        image_sizes.append(img.size)\n",
        "    \n",
        "    avg_width = np.mean([size[0] for size in image_sizes])\n",
        "    avg_height = np.mean([size[1] for size in image_sizes])\n",
        "    \n",
        "    return {\n",
        "        'split': split_name,\n",
        "        'samples': len(split_pairs),\n",
        "        'total_objects': total_objects,\n",
        "        'avg_objects_per_image': total_objects / len(split_pairs) if split_pairs else 0,\n",
        "        'avg_width': avg_width,\n",
        "        'avg_height': avg_height\n",
        "    }\n",
        "\n",
        "# Analyze all splits\n",
        "train_stats = analyze_dataset(train_pairs, 'train')\n",
        "val_stats = analyze_dataset(val_pairs, 'val')\n",
        "test_stats = analyze_dataset(test_pairs, 'test')\n",
        "\n",
        "# Create summary DataFrame\n",
        "stats_df = pd.DataFrame([train_stats, val_stats, test_stats])\n",
        "print(\"Dataset Statistics:\")\n",
        "print(stats_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize dataset distribution\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Sample distribution pie chart\n",
        "axes[0, 0].pie([len(train_pairs), len(val_pairs), len(test_pairs)], \n",
        "               labels=['Train (90%)', 'Val (5%)', 'Test (5%)'], \n",
        "               autopct='%1.1f%%', startangle=90)\n",
        "axes[0, 0].set_title('Dataset Split Distribution')\n",
        "\n",
        "# Object count per split\n",
        "splits = ['Train', 'Val', 'Test']\n",
        "object_counts = [train_stats['total_objects'], val_stats['total_objects'], test_stats['total_objects']]\n",
        "axes[0, 1].bar(splits, object_counts, color=['blue', 'orange', 'green'])\n",
        "axes[0, 1].set_title('Total Objects per Split')\n",
        "axes[0, 1].set_ylabel('Number of Objects')\n",
        "\n",
        "# Average objects per image\n",
        "avg_objects = [train_stats['avg_objects_per_image'], val_stats['avg_objects_per_image'], test_stats['avg_objects_per_image']]\n",
        "axes[1, 0].bar(splits, avg_objects, color=['blue', 'orange', 'green'])\n",
        "axes[1, 0].set_title('Average Objects per Image')\n",
        "axes[1, 0].set_ylabel('Average Objects')\n",
        "\n",
        "# Image size distribution\n",
        "all_sizes = []\n",
        "for split_pairs in [train_pairs, val_pairs, test_pairs]:\n",
        "    for img_path, _ in split_pairs:\n",
        "        img = Image.open(img_path)\n",
        "        all_sizes.append(img.size)\n",
        "\n",
        "widths = [size[0] for size in all_sizes]\n",
        "heights = [size[1] for size in all_sizes]\n",
        "axes[1, 1].scatter(widths, heights, alpha=0.6)\n",
        "axes[1, 1].set_xlabel('Width (pixels)')\n",
        "axes[1, 1].set_ylabel('Height (pixels)')\n",
        "axes[1, 1].set_title('Image Size Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample images with annotations\n",
        "def display_sample_images(split_pairs, split_name, num_samples=4):\n",
        "    \"\"\"Display sample images with their annotations\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    sample_pairs = random.sample(split_pairs, min(num_samples, len(split_pairs)))\n",
        "    \n",
        "    for idx, (img_path, lbl_path) in enumerate(sample_pairs):\n",
        "        # Load image\n",
        "        img = cv2.imread(str(img_path))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        h, w = img.shape[:2]\n",
        "        \n",
        "        # Load annotations\n",
        "        with open(lbl_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        \n",
        "        # Draw bounding boxes\n",
        "        for line in lines:\n",
        "            if line.strip():\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) >= 5:\n",
        "                    class_id, x_center, y_center, width, height = map(float, parts[:5])\n",
        "                    \n",
        "                    # Convert from normalized coordinates to pixel coordinates\n",
        "                    x_center *= w\n",
        "                    y_center *= h\n",
        "                    width *= w\n",
        "                    height *= h\n",
        "                    \n",
        "                    # Calculate corner coordinates\n",
        "                    x1 = int(x_center - width/2)\n",
        "                    y1 = int(y_center - height/2)\n",
        "                    x2 = int(x_center + width/2)\n",
        "                    y2 = int(y_center + height/2)\n",
        "                    \n",
        "                    # Draw rectangle\n",
        "                    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "                    cv2.putText(img, 'balloon', (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
        "        \n",
        "        axes[idx].imshow(img)\n",
        "        axes[idx].set_title(f'{split_name}: {img_path.name}')\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for idx in range(len(sample_pairs), len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.suptitle(f'Sample Images from {split_name} Split', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display samples from each split\n",
        "display_sample_images(train_pairs, 'Training', 4)\n",
        "display_sample_images(val_pairs, 'Validation', 4)\n",
        "display_sample_images(test_pairs, 'Test', 4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Training Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "training_config = {\n",
        "    'model': 'yolo11n.pt',  # YOLOv11 nano for speed\n",
        "    'data': str(custom_data_yaml),\n",
        "    'epochs': 100,\n",
        "    'imgsz': 640,\n",
        "    'batch': 16,\n",
        "    'patience': 20,  # Early stopping\n",
        "    'dropout': 0.1,  # Regularization\n",
        "    'lr0': 0.01,     # Learning rate\n",
        "    'momentum': 0.937,\n",
        "    'weight_decay': 0.0005,\n",
        "    'warmup_epochs': 3,\n",
        "    'cos_lr': True,  # Cosine learning rate scheduler\n",
        "    'save_period': 10,\n",
        "    'project': 'runs/train',\n",
        "    'name': 'roboflow_balloon_90_5_5',\n",
        "    \n",
        "    # GPU/CUDA settings\n",
        "    'device': 0 if torch.cuda.is_available() else 'cpu',  # Use GPU if available\n",
        "    'workers': 8 if torch.cuda.is_available() else 4,     # More workers for GPU\n",
        "    \n",
        "    # Data augmentation\n",
        "    'hsv_h': 0.015,\n",
        "    'hsv_s': 0.7,\n",
        "    'hsv_v': 0.4,\n",
        "    'degrees': 0.0,\n",
        "    'translate': 0.1,\n",
        "    'scale': 0.5,\n",
        "    'shear': 0.0,\n",
        "    'perspective': 0.0,\n",
        "    'flipud': 0.0,\n",
        "    'fliplr': 0.5,\n",
        "    'mosaic': 1.0,\n",
        "    'mixup': 0.0,\n",
        "    \n",
        "    # Model settings\n",
        "    'save': True,\n",
        "    'save_txt': True,\n",
        "    'save_conf': True,\n",
        "    'save_crop': False,\n",
        "    'show_labels': True,\n",
        "    'show_conf': True,\n",
        "    'visualize': False,\n",
        "    'augment': True,\n",
        "    'agnostic_nms': False,\n",
        "    'retina_masks': False,\n",
        "    'overlap_mask': True,\n",
        "    'mask_ratio': 4,\n",
        "}\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "for key, value in training_config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(f\"\\nüöÄ Training will use: {'GPU (CUDA)' if torch.cuda.is_available() else 'CPU'}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   ‚Ä¢ Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   ‚Ä¢ Workers: {training_config['workers']}\")\n",
        "else:\n",
        "    print(f\"   ‚Ä¢ Workers: {training_config['workers']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model = YOLO(training_config['model'])\n",
        "print(f\"‚úÖ Loaded model: {training_config['model']}\")\n",
        "\n",
        "# Start training\n",
        "print(\"\\nüöÄ Starting training...\")\n",
        "print(f\"Dataset: {training_config['data']}\")\n",
        "print(f\"Epochs: {training_config['epochs']}\")\n",
        "print(f\"Batch size: {training_config['batch']}\")\n",
        "print(f\"Image size: {training_config['imgsz']}\")\n",
        "\n",
        "# Train the model\n",
        "results = model.train(**training_config)\n",
        "\n",
        "print(\"\\n‚úÖ Training completed!\")\n",
        "print(f\"Results saved to: runs/train/{training_config['name']}/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training Results Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training results\n",
        "results_path = Path(f\"runs/train/{training_config['name']}\")\n",
        "results_csv = results_path / \"results.csv\"\n",
        "\n",
        "if results_csv.exists():\n",
        "    # Load results\n",
        "    results_df = pd.read_csv(results_csv)\n",
        "    \n",
        "    print(\"Training Results Summary:\")\n",
        "    print(f\"Total epochs: {len(results_df)}\")\n",
        "    print(f\"Best mAP50: {results_df['metrics/mAP50(B)'].max():.4f}\")\n",
        "    print(f\"Best mAP50-95: {results_df['metrics/mAP50-95(B)'].max():.4f}\")\n",
        "    print(f\"Final loss: {results_df['train/box_loss'].iloc[-1]:.4f}\")\n",
        "    \n",
        "    # Display last few epochs\n",
        "    print(\"\\nLast 5 epochs:\")\n",
        "    print(results_df[['epoch', 'train/box_loss', 'val/box_loss', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']].tail())\n",
        "else:\n",
        "    print(\"Results CSV not found. Training may still be in progress.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "if results_csv.exists():\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Loss curves\n",
        "    axes[0, 0].plot(results_df['epoch'], results_df['train/box_loss'], label='Train Box Loss', color='blue')\n",
        "    axes[0, 0].plot(results_df['epoch'], results_df['val/box_loss'], label='Val Box Loss', color='red')\n",
        "    axes[0, 0].set_title('Box Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "    \n",
        "    # mAP curves\n",
        "    axes[0, 1].plot(results_df['epoch'], results_df['metrics/mAP50(B)'], label='mAP@0.5', color='green')\n",
        "    axes[0, 1].plot(results_df['epoch'], results_df['metrics/mAP50-95(B)'], label='mAP@0.5:0.95', color='orange')\n",
        "    axes[0, 1].set_title('mAP Metrics')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('mAP')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "    \n",
        "    # Precision and Recall\n",
        "    axes[1, 0].plot(results_df['epoch'], results_df['metrics/precision(B)'], label='Precision', color='purple')\n",
        "    axes[1, 0].plot(results_df['epoch'], results_df['metrics/recall(B)'], label='Recall', color='brown')\n",
        "    axes[1, 0].set_title('Precision & Recall')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Score')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True)\n",
        "    \n",
        "    # Learning rate\n",
        "    if 'lr/pg0' in results_df.columns:\n",
        "        axes[1, 1].plot(results_df['epoch'], results_df['lr/pg0'], label='Learning Rate', color='red')\n",
        "        axes[1, 1].set_title('Learning Rate Schedule')\n",
        "        axes[1, 1].set_xlabel('Epoch')\n",
        "        axes[1, 1].set_ylabel('Learning Rate')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Cannot plot training curves - results CSV not found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Model Validation and Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "best_model_path = results_path / \"weights\" / \"best.pt\"\n",
        "if best_model_path.exists():\n",
        "    best_model = YOLO(str(best_model_path))\n",
        "    print(f\"‚úÖ Loaded best model: {best_model_path}\")\n",
        "    \n",
        "    # Validate on test set\n",
        "    print(\"\\nüîç Validating on test set...\")\n",
        "    test_results = best_model.val(data=str(custom_data_yaml), split='test')\n",
        "    \n",
        "    print(f\"Test Results:\")\n",
        "    print(f\"  mAP@0.5: {test_results.box.map50:.4f}\")\n",
        "    print(f\"  mAP@0.5:0.95: {test_results.box.map:.4f}\")\n",
        "    print(f\"  Precision: {test_results.box.mp:.4f}\")\n",
        "    print(f\"  Recall: {test_results.box.mr:.4f}\")\n",
        "else:\n",
        "    print(\"Best model not found. Training may not be complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Inference on Sample Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run inference on sample images\n",
        "if best_model_path.exists():\n",
        "    # Get sample images from test set\n",
        "    test_images = list((custom_dataset_path / 'test' / 'images').glob('*.jpg'))[:4]\n",
        "    \n",
        "    if test_images:\n",
        "        print(f\"Running inference on {len(test_images)} test images...\")\n",
        "        \n",
        "        # Run inference\n",
        "        results = best_model(test_images, save=True, save_txt=True, conf=0.25)\n",
        "        \n",
        "        # Display results\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        axes = axes.flatten()\n",
        "        \n",
        "        for idx, result in enumerate(results):\n",
        "            if idx < len(axes):\n",
        "                # Get the image with predictions\n",
        "                img = result.plot()\n",
        "                axes[idx].imshow(img)\n",
        "                axes[idx].set_title(f'Prediction: {result.path.name}')\n",
        "                axes[idx].axis('off')\n",
        "                \n",
        "                # Print detection info\n",
        "                if result.boxes is not None and len(result.boxes) > 0:\n",
        "                    print(f\"{result.path.name}: {len(result.boxes)} detections\")\n",
        "                    for box in result.boxes:\n",
        "                        conf = box.conf.item()\n",
        "                        print(f\"  Confidence: {conf:.3f}\")\n",
        "                else:\n",
        "                    print(f\"{result.path.name}: No detections\")\n",
        "        \n",
        "        # Hide unused subplots\n",
        "        for idx in range(len(results), len(axes)):\n",
        "            axes[idx].axis('off')\n",
        "        \n",
        "        plt.suptitle('Model Predictions on Test Images', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No test images found for inference.\")\n",
        "else:\n",
        "    print(\"Cannot run inference - best model not found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Model Performance Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create performance summary\n",
        "if results_csv.exists() and best_model_path.exists():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üéØ BALLOON DETECTION MODEL - PERFORMANCE SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(f\"\\nüìä Dataset Information:\")\n",
        "    print(f\"  ‚Ä¢ Total samples: {total_samples}\")\n",
        "    print(f\"  ‚Ä¢ Training samples: {len(train_pairs)} (90%)\")\n",
        "    print(f\"  ‚Ä¢ Validation samples: {len(val_pairs)} (5%)\")\n",
        "    print(f\"  ‚Ä¢ Test samples: {len(test_pairs)} (5%)\")\n",
        "    print(f\"  ‚Ä¢ Total objects: {train_stats['total_objects'] + val_stats['total_objects'] + test_stats['total_objects']}\")\n",
        "    \n",
        "    print(f\"\\nüèãÔ∏è Training Configuration:\")\n",
        "    print(f\"  ‚Ä¢ Model: {training_config['model']}\")\n",
        "    print(f\"  ‚Ä¢ Epochs: {training_config['epochs']}\")\n",
        "    print(f\"  ‚Ä¢ Batch size: {training_config['batch']}\")\n",
        "    print(f\"  ‚Ä¢ Image size: {training_config['imgsz']}\")\n",
        "    print(f\"  ‚Ä¢ Learning rate: {training_config['lr0']}\")\n",
        "    print(f\"  ‚Ä¢ Early stopping patience: {training_config['patience']}\")\n",
        "    \n",
        "    print(f\"\\nüìà Training Results:\")\n",
        "    print(f\"  ‚Ä¢ Best mAP@0.5: {results_df['metrics/mAP50(B)'].max():.4f}\")\n",
        "    print(f\"  ‚Ä¢ Best mAP@0.5:0.95: {results_df['metrics/mAP50-95(B)'].max():.4f}\")\n",
        "    print(f\"  ‚Ä¢ Final training loss: {results_df['train/box_loss'].iloc[-1]:.4f}\")\n",
        "    print(f\"  ‚Ä¢ Final validation loss: {results_df['val/box_loss'].iloc[-1]:.4f}\")\n",
        "    \n",
        "    if 'test_results' in locals():\n",
        "        print(f\"\\nüß™ Test Set Performance:\")\n",
        "        print(f\"  ‚Ä¢ mAP@0.5: {test_results.box.map50:.4f}\")\n",
        "        print(f\"  ‚Ä¢ mAP@0.5:0.95: {test_results.box.map:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Precision: {test_results.box.mp:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Recall: {test_results.box.mr:.4f}\")\n",
        "    \n",
        "    print(f\"\\nüíæ Model Files:\")\n",
        "    print(f\"  ‚Ä¢ Best model: {best_model_path}\")\n",
        "    print(f\"  ‚Ä¢ Last model: {results_path / 'weights' / 'last.pt'}\")\n",
        "    print(f\"  ‚Ä¢ Results: {results_path}\")\n",
        "    \n",
        "    print(f\"\\nüéØ Air Defense System Readiness:\")\n",
        "    best_map50 = results_df['metrics/mAP50(B)'].max()\n",
        "    if best_map50 > 0.8:\n",
        "        print(f\"  ‚úÖ Excellent detection performance (mAP@0.5: {best_map50:.3f})\")\n",
        "    elif best_map50 > 0.6:\n",
        "        print(f\"  ‚úÖ Good detection performance (mAP@0.5: {best_map50:.3f})\")\n",
        "    elif best_map50 > 0.4:\n",
        "        print(f\"  ‚ö†Ô∏è  Moderate detection performance (mAP@0.5: {best_map50:.3f})\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå Poor detection performance (mAP@0.5: {best_map50:.3f})\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Training completed successfully! üöÄ\")\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Training results not available. Please check if training completed successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Export Model for Deployment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export model for deployment\n",
        "if best_model_path.exists():\n",
        "    print(\"üì¶ Exporting model for deployment...\")\n",
        "    \n",
        "    # Export to different formats\n",
        "    export_formats = ['onnx', 'torchscript']\n",
        "    \n",
        "    for fmt in export_formats:\n",
        "        try:\n",
        "            exported_path = best_model.export(format=fmt)\n",
        "            print(f\"  ‚úÖ Exported to {fmt}: {exported_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Failed to export to {fmt}: {e}\")\n",
        "    \n",
        "    print(\"\\nüéØ Model ready for air defense system deployment!\")\n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"1. Integrate with tracking system (SORT)\")\n",
        "    print(\"2. Add friend/foe color classification\")\n",
        "    print(\"3. Implement safety gates and no-fire zones\")\n",
        "    print(\"4. Deploy on edge device (RPi5/Jetson)\")\n",
        "else:\n",
        "    print(\"Cannot export model - best model not found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéØ **Notebook Complete!**\n",
        "\n",
        "This notebook provides a complete pipeline for training a balloon detection model on the Roboflow dataset with your requested 90/5/5 split. The trained model is ready for integration into your air defense system.\n",
        "\n",
        "**Key Features:**\n",
        "- ‚úÖ Roboflow dataset download and processing\n",
        "- ‚úÖ Custom 90/5/5 train/test/validation split\n",
        "- ‚úÖ Comprehensive dataset analysis and visualization\n",
        "- ‚úÖ YOLOv11 training with optimized parameters\n",
        "- ‚úÖ Training metrics and performance analysis\n",
        "- ‚úÖ Model validation and inference testing\n",
        "- ‚úÖ Model export for deployment (ONNX/TorchScript)\n",
        "\n",
        "**Ready for your air defense demo system! üöÄ**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
